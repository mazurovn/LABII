# -*- coding: utf-8 -*-
"""Работа 10 Синтаксис pytorch basics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_zvHdkgyZH3oPwkdAMvK348E2GjhETwf

---

<h2 style="text-align: center;"><b>PyTorch. Основы: синтаксис, torch.cuda и torch.autograd</b></h2>

---

<p style="align: center;"><img src="https://upload.wikimedia.org/wikipedia/commons/9/96/Pytorch_logo.png" width=500 height=400></p>

На этом занятии мы рассмотрим основы фреймворка глубокого обучения PyTorch.  

Когда хочется написать какую-нибудь нейросеть, решающую определённую задачу, будь то какая-нибудь простая классификация чего-либо или обнаружение лиц людей на видео. Всё, конечно, всегда начинается со **сбора данных**, а уже потом реализуются модели и проводятся эксперименты.  

Однако люди быстро поняли, что писать свои нейронные сети каждый раз с нуля ну очень уж долго и неразумно, поэтому придумали так называемые **фреймворки** - модули, в которых есть функционал, с помощью которого можно быстро и просто решать типовые задачи, и уже с помощью этих средств писать решения к более сложным задачам.

Есть много различных фремворков глубокого обучения. Разница между ними прежде всего в том, каков общий принцип вычислений.
Например, в **Caffe и Caffe2** вы пишете код, по сути, составляя его из готовых "кусочков", как в Lego, в **TensorFlow и Theano** вы сначала объявляете вычислительный граф, потом компилируете его и запускаете (sees.run()), в то время как в **Torch и PyTorch** вы пишете почти точно так же, как на NumPy, а граф вычислений создаётся только при запуске (то есть существует только во время выполнения, потом он "разрушается"). **Keras** позволяет как строить блоки, так и компилировать свой граф:

<p style="align: center;"><img src="https://habrastorage.org/web/e3e/c3e/b78/e3ec3eb78d714a7993a6b922911c0866.png" width=500 height=500></p>  
<p style="text-align: center;"><i>Картинка взята из этой [статьи на Хабре](https://habr.com/post/334380/)</i><p>

<h3 style="text-align: center;"><b>Установка</b></h3>

Инструкция по установке PyTorch есть на [официальном сайте PyTorch](https://pytorch.org/).

<h3 style="text-align: center;">Синтаксис<b></b></h3>
"""

import torch

"""Сначала немного фактов про PyTorch:  
- динамический граф вычислений
- удобные модули `torch.nn` и `torchvision` для написания нейросетей с минимальными усилиями
- в некоторых задачах даже быстрее TensorFlow
- легко проводить вычисления на GPU

Если PyTorch представить формулой, то она будет такой:  

$$PyTorch = NumPy + CUDA + Autograd$$

(CUDA - [wiki](https://ru.wikipedia.org/wiki/CUDA))

Посмотрим, как в PyTorch выполняются операции с векторами.  

Напоминание: **тензором** называется многомерный вектор, то есть:  

x = np.array([1,2,3]) - вектор = тензор размерности 1 (то есть (1,))  
y = np.array([[1, 2, 3], [4, 5, 6]]) - матрица = тензор размерности 2 (в данном случае тензор (2, 3))  
z = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]],  
    [[1, 2, 3], [4, 5, 6], [7, 8, 9]],  
              [[1, 2, 3], [4, 5, 6], [7, 8, 9]]]) - "кубик" (3, 3, 3) = тензор размерности 3 (в данном случае (3, 3, 3))

Простейшим примером 3-мерного тензора является **картинка** - это "параллелепипед" из чисел, у которого три размерности - высота, ширина и количество каналов, значит это тензор размерности 3.

Понятие тензора нужно знать потому, что в PyTorch мы оперируем переменными типа `torch.Tensor` (`FloatTensor`, `IntTensor`, `ByteTensor`), и пугаться их названий совершенно не нужно - это просто векторы, у которых несколько размерностей.

Все типы тензоров:
"""

torch.HalfTensor      # 16 бит, с плавающей точкой
torch.FloatTensor     # 32 бита,  с плавающей точкой
torch.DoubleTensor    # 64 бита, с плавающей точкой

torch.ShortTensor     # 16 бит, целочисленный, знаковый
torch.IntTensor       # 32 бита, целочисленный, знаковый
torch.LongTensor      # 64 бита, целочисленный, знаковый

torch.CharTensor      # 8 бит, целочисленный, знаковый
torch.ByteTensor      # 8 бит, целочисленный, беззнаковый

"""Мы будем использовать только `torch.FloatTensor()` и `torch.IntTensor()`.

Перейдём к делу:

* Создание тензоров:
"""

a = torch.FloatTensor([1, 2])
a

a.shape

b = torch.FloatTensor([[1,2,3], [4,5,6]])
b

b.shape

x = torch.FloatTensor(2,3,4)

x

x = torch.FloatTensor(100)
x

x = torch.IntTensor(45, 57, 14, 2)
x.shape

"""**Обратите внимание** - если вы создаёте тензор через задание размерностей (как в примере выше), то он изначально заполняюстя случайным "мусором". Что инициализировать нулями, нужно написать .zero_() в конце:"""

x = torch.IntTensor(3, 2, 4)
x

x = torch.IntTensor(3, 2, 4).zero_()
x

"""Аналог функции `np.reshape()` == `torch.view()`:"""

b.view(3, 2)

b

"""**Обратите внимание** - torch.view() создаёт новый тензор, а не изменяет старый!"""

b.view(-1)

b

"""* Изменение типа тензора:"""

a = torch.FloatTensor([1.5, 3.2, -7])

a.type_as(torch.IntTensor())

a.type_as(torch.ByteTensor())

"""Обратите внимание, что при `.type_as()` создаётся новый тензор (старый не меняется), то есть это не in-place операция:"""

a

"""* Индексация точная такая же, как и в NumPy:"""

a = torch.FloatTensor([[100, 20, 35], [15, 163, 534], [52, 90, 66]])
a

a[0, 0]

a[0][0]

a[0:2, 0:2]

"""**Арифметика и булевы операции** работаю также, как и в NumPy, **НО** лучше использовать не опреаторы `+`, `-`, `*`, `/`, а их аналоги:  

| Оператор | Аналог |
|:-:|:-:|
|`+`| `torch.add()` |
|`-`| `torch.sub()` |
|`*`| `torch.mul()` |
|`/`| `torch.div()` |

* Сложение:
"""

a = torch.FloatTensor([[1, 2, 3], [10, 20, 30], [100, 200, 300]])
b = torch.FloatTensor([[-1, -2, -3], [-10, -20, -30], [100, 200, 300]])

a + b

"""Лучше:"""

a.add(b)

b = -a
b

a + b

"""* Вычитание:"""

a - b

"""Лучше:"""

a.sub(b)

"""* Умножение (поэлементное):"""

a * b

"""Лучше:"""

a.mul(b)

"""* Деление (поэлементное):"""

a = torch.FloatTensor([[1, 2, 3], [10, 20, 30], [100, 200, 300]])
b = torch.FloatTensor([[-1, -2, -3], [-10, -20, -30], [100, 200, 300]])

a / b

"""Лучше:"""

a.div(b)

"""Заметьте, все эти операции **не меняют исходные тензоры**, а **создают новые**:"""

a

b

"""* **Операторы сравнения**:"""

a = torch.FloatTensor([[1, 2, 3], [10, 20, 30], [100, 200, 300]])
b = torch.FloatTensor([[-1, -2, -3], [-10, -20, -30], [100, 200, 300]])

a == b

a != b

a < b

a > b

"""* **Булевы маски**:"""

a[a > b]

b[a == b]

"""Опять же, тензоры не меняются:"""

a

b

"""Применение **стандартных функций** такое же, как и в numpy - поэлементное:"""

a = torch.FloatTensor([[1, 2, 3], [10, 20, 30], [100, 200, 300]])

a.sin()

torch.sin(a)

a.cos()

a.exp()

a.log()

b = -a
b

b.abs()

"""**Сумма, среднее, максимум, минимум**:"""

a.sum()

a.mean()

"""По осям:"""

a.sum(0)

a.sum(1)

a.max()

a.max(0)

a.min()

a.min(0)

"""Обратите внимание - второй тензор при вызове функций .max() и .min() - это индексы этих максимальных/минимальных элементов по указанной размерности (то есть в данном случае a.min() вернул (1, 2, 3) - минимумы по 0 оси (по столбцам), и их индексы по 0-ой оси (0,0,0) (номер каждого элемента в своём столбце)).

**Матричные операции:**

* Транспонирование матрицы (тензора):
"""

a = torch.FloatTensor([[1, 2, 3], [10, 20, 30], [100, 200, 300]])
a

a.t()

"""И снова - сам тензор не меняется (то есть при вызове создаётся новый):"""

a

"""* Скалярное произведение векторов (1-мерных тензоров):"""

a = torch.FloatTensor([1, 2, 3, 4, 5, 6])
b = torch.FloatTensor([-1, -2, -4, -6, -8, -10])

a.dot(b)

a @ b

type(a)

type(b)

type(a @ b)

"""* Матричное умножение:"""

a = torch.FloatTensor([[1, 2, 3], [10, 20, 30], [100, 200, 300]])
b = torch.FloatTensor([[-1, -2, -3], [-10, -20, -30], [100, 200, 300]])

a.mm(b)

a @ b

"""Тензоры неизменны:"""

a

b

a = torch.FloatTensor([[1, 2, 3], [10, 20, 30], [100, 200, 300]])
b = torch.FloatTensor([[-1], [-10], [100]])

print(a.shape, b.shape)

a @ b

"""Если "развернуть" тензор b просто в массив элементов (`torch.view(-1)`), умножение будет как на столбец:"""

b

b.view(-1)

a @ b.view(-1)

a.mv(b.view(-1))

"""**Перевод из NumPy в PyTorch**:"""

import numpy as np

a = np.random.rand(3, 3)
a

b = torch.from_numpy(a)
b

"""**НО!** Обратите внимание - a и b в этом случае будут использовать одно и то же хранилище данных, то есть измение одного тензора будет менять и другой:"""

b -= b
b

a

"""**Перевод из PyTorch в NumPy:**"""

a = torch.FloatTensor(2, 3, 4)
a

type(a)

x = a.numpy()
x

x.shape

type(x)

"""Напишем функцию `forward_pass(X, w)` ($w_0$ входит в $w$) для одного нейрона (с сигмоидой) с помощью PyTorch:"""

def forward_pass(X, w):
    return torch.sigmoid(X @ w)

X = torch.FloatTensor([[-5, 5], [2, 3], [1, -1]])
w = torch.FloatTensor([[-0.5], [2.5]])
result = forward_pass(X, w)
print('result: {}'.format(result))

"""<h3 style="text-align: center;"><a href="https://ru.wikipedia.org/wiki/CUDA">CUDA</a></h3>

[Краткое видео про то, как GPU используется в обучении нейросетей](https://www.youtube.com/watch?v=EobhK0UZm80)

Все вычисления в PyTorch можно проводить как на CPU, так и на GPU (Graphical Processing Unit) (если она у вас есть). В PyTorch переключение между ними делается очень просто, что является одной из ключевых его особенностей.
"""

x = torch.FloatTensor(1024, 1024).uniform_()
x

x.is_cuda

"""Переместим на GPU:"""

x = x.cuda()

x.is_cuda

x

"""Перемножим две тензора на GPu и вернём результат вычисления на CPU:"""

a = torch.FloatTensor(10000, 10000).uniform_()
b = torch.FloatTensor(10000, 10000).uniform_()
c = a.cuda().mul(b.cuda()).cpu()

c

a

"""Тензоры, лежащие на CPU, и тензоры, лежащие на GPU, недоступны друг для друга:"""

a = torch.FloatTensor(10000, 10000).uniform_().cpu()
b = torch.FloatTensor(10000, 10000).uniform_().cuda()

a + b

"""Вот ещё немного про то, как можно работать с GPU:"""

x = torch.FloatTensor(5, 5, 5).uniform_()

# проверяем, есть ли CUDA (то есть NVidia GPU)
if torch.cuda.is_available():
    # так можно получить имя устройства, которое связано с CUDA
    # (полезно в случае с несколькими видеокартами)
    device = torch.device('cuda')          # CUDA-device объект
    y = torch.ones_like(x, device=device)  # создаём тензор на GPU
    x = x.to(device)                       # тут можно просто ``.to("cuda")``
    z = x + y
    print(z)
    # с помощью``.to`` можно и изменить тип при перемещении
    print(z.to("cpu", torch.double))

"""<h3 style="text-align: center;">Autograd<b></b></h3>

Расшифровывается как Automatic Gradients (автоматическое взятие градиентов) - собственно, из названия понятно, что это модуль PyTorch, отвечающий за взятие производных.  

Возможно, для вас это бдет шок, но PyTorch (и любой фреймворк глубокого обучения) может продифференцировать функцию практически любой сложности.

Импортируем нужный класс:
"""

from torch.autograd import Variable

"""Идея такая: оборачиваем тензор в класс Variable(), получаем тоже тензор, но он имеет способность вычислять себе градиенты.

Если а - тензор, обёрнутый в Variable(), то при вызове a.backward() берутся градиенты по всем переменным, от которых зависит тензор a.

**Примечание:** Если вы используете версию `pytorch 0.4.0` или более новую, то ***`torch.Tensor` позволяет брать по нему градиенты, класс `Variable()` использовать не нужно.*** (`torch.Variable()` - deprecated).

Примеры:
"""

x = torch.FloatTensor(3, 1).uniform_()
y = torch.FloatTensor(3, 1).uniform_()
w = torch.FloatTensor(3, 3).uniform_()
b = torch.FloatTensor(3, 1).uniform_()

x = Variable(x, requires_grad=True)
y = Variable(x, requires_grad=False)
w = Variable(w, requires_grad=True)
b = Variable(b, requires_grad=True)

y_pred = (w @ x).add_(b)

loss = (y_pred - y).sum()

# берём градиенты
loss.backward()

x.grad

w.grad

b.grad

y.grad

"""**Обратите внимание** - градиенты лежат в поле `.grad` у тех тензоров (Variable'ов), по которым брали эти градиенты. Градиенты **не лежат** в той Variable, от которой они брались!

Получить тензор из `Variable()` можно с помощью поля `.data`:
"""

x

x.data

"""<h3 style="text-align: center;">Полезные ссылки:<b></b></h3>

*1). Статья по PyTorch (на русском): https://habr.com/post/334380/*

*2). Туториалы от самих разработчиков фреймворка: https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py*

*3). Статья на arXiv о сравнении фреймворков глубокого обучения: https://arxiv.org/pdf/1511.06435.pdf*

4). *Ещё туториалы: https://github.com/yunjey/pytorch-tutorial*

*5). Сайт Facebook AI Research - отдела, который разрабатывает PyTorch и другие активно вкладывается в разработку инструментов для AI: https://facebook.ai/developers/tools*
"""