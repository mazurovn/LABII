{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1 часть. Линейные методы, аналитическое решение.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNK_0ITKDx_c"
      },
      "source": [
        "# Часть 1. Линейные методы. Регрессия."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В ноутбуке:\n",
        "- Решим задачу линейной регрессии аналитически\n",
        "- Решим задачу линейной регрессии численно, используя библиотеку sklearn"
      ],
      "metadata": {
        "id": "nIg2Q3Q_ILLW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubtW4BNLIC0R"
      },
      "source": [
        "ссылка на ноутбук в колаб: https://colab.research.google.com/drive/1XF0JaI-klJF1KXzEICSrPXGpzQATnN9E?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Немного теории"
      ],
      "metadata": {
        "id": "pxCPo4XSIcUn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRVu5ANBLGGo"
      },
      "source": [
        "Линейные методы предполагают, что между признаками объекта и целевой переменной существует линейная зависимость, то есть:\n",
        "$$ y = w_1 x_1 + w_2 x_2 + ... + w_k x_k + b $$,\n",
        "где у - целевая переменная (что мы хотим предсказать), $x_i$ -- признак объекта х, $w_i$ -- вес i-го признака, b -- bias (смещение, свободный член)\n",
        "\n",
        "Часто предполагают, что объект х содержит в себе фиктивный признак, который всегда равен 1, тогда bias это есть вес этого признака. В этом случае формула принимает простой вид:\n",
        "$$ y = <w, x> $$,\n",
        "где $<\\cdot, \\cdot>$ -- скалярное произведение векторов.\n",
        "\n",
        "В матричной форме, в случае, когда у нас есть n объектов формулу можно переписать следующим образом:\n",
        "$$ y = Xw $$,\n",
        "y -- вектор размера n, X -- матрица объекты-признаки размера $n \\times k$, w -- вектор весов размера k.\n",
        "\n",
        "Решение по методу наименьших квадратов дает \n",
        "$$ w = (X^TX)^{-1}X^Ty $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx_zFZBo6iKb"
      },
      "source": [
        "**Определение (Lp-норма):**\n",
        "\n",
        "$$\n",
        "    \\|\\cdot\\|_{p}: \\mathbb{R}^{d} \\to \\mathbb{R}\\\\\n",
        "    \\forall p \\geq 1: \\forall x \\in \\mathbb{R}^{d}: \\|x\\|_{p} = \\sqrt[p]{\\sum_{i=1}^{n} x_{i}^{p}}\n",
        "$$\n",
        "\n",
        "**Доказательство**\n",
        "\n",
        "Вспомним, как выглядит задача оптимизации:\n",
        "\n",
        "$$\n",
        "    \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\langle x_i, w \\rangle)^2 \\to \\min\\limits_{w}\n",
        "$$\n",
        "\n",
        "Эта задача оптимизации допускает следующую более удобную запись:\n",
        "\n",
        "$$\n",
        "    \\frac{1}{n} \\| Xw - y \\|_{2}^{2} \\to \\min\\limits_{w}\n",
        "$$\n",
        "\n",
        "Утверждается, что:\n",
        "\n",
        "$$\n",
        "    \\frac{1}{n} \\| Xw - y \\|_{2}^{2} = \\frac{1}{n} (Xw - y)^{\\top} (Xw - y)\n",
        "$$\n",
        "\n",
        "(потому что $\\| x \\|_{2}^{2} = \\langle x, x\\rangle$)\n",
        "\n",
        "Раскроем это выражение:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "    & (Xw - y)^{\\top} (Xw - y) =\\\\\n",
        "    &= (w^{\\top} X^{\\top} - y^{\\top}) (Xw - y) =\\\\\n",
        "    &= (w^{\\top} X^{\\top} X w - w^{\\top} X^{\\top} y) - (y^{\\top} X w - y^{\\top} y) =\\\\\n",
        "    &= w^{\\top} X^{\\top} X w - 2 y^{\\top} X w + y^{\\top} y\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Найдём градиент этой функции, т.е. все частные производные по весам (т.е. по $w_1, \\ldots, w_d$).\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "    &\\frac{\\partial}{\\partial w} (w^{\\top} X^{\\top} X w - 2 y^{\\top} X w + y^{\\top} y) =\\\\\n",
        "    &= 2 X^{\\top} X w - 2 X^{\\top} y = 0\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Отсюда получаем итоговый ответ:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "    X^{\\top} X w &= X^{\\top} y\\\\\n",
        "    w &= (X^{\\top} X)^{-1} X^{\\top} y\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOt0ORJluEhQ"
      },
      "source": [
        "Полезная статья прорешение Линейной регрессии: https://habr.com/ru/post/474602/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9dsBCqULS3F"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss3Xirk33T5x"
      },
      "source": [
        "X = np.linspace(-5, 5, 100)\n",
        "y = 10 * X - 7\n",
        "\n",
        "\n",
        "X_train = X[0::2].reshape(-1, 1)\n",
        "y_train = y[0::2] + np.random.randn(50) * 10\n",
        "\n",
        "X_test = X[1::2].reshape(-1, 1)\n",
        "y_test = y[1::2] + np.random.randn(50) * 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTCJ6Jc2LP3h"
      },
      "source": [
        "X_train[1], y_train[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV_yTmiAMcZc"
      },
      "source": [
        "def fit(X, y):\n",
        "    n, k = X.shape\n",
        "    X = np.hstack((X, np.ones((n, 1))))\n",
        "    w = np.linalg.inv(X.T @ X) @ X.T @ y\n",
        "    return w\n",
        "\n",
        "def predict(X, w):\n",
        "    n, k = X.shape\n",
        "    X = np.hstack((X, np.ones((n, 1))))\n",
        "    y_pred = X @ w\n",
        "    return y_pred\n",
        "\n",
        "weights = fit(X_train, y_train)\n",
        "weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDM9jvhOyFp8"
      },
      "source": [
        "y_hat = predict(X_test, weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSr7D-CsyRXY"
      },
      "source": [
        "plt.hist((y_test - y_hat)**2, bins = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JIVQm0J1bde"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(X, y, label = 'data')\n",
        "plt.scatter(X_train, y_train, label ='train')\n",
        "plt.scatter(X_test, y_test, label ='test')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwSr7vva14-I"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(X, y, label = 'data')\n",
        "plt.scatter(X_train, y_train, label ='train')\n",
        "plt.scatter(X_test, y_test, label ='test')\n",
        "plt.plot(X[1::2], X[1::2].reshape(-1, 1).dot(weights[:-1]) + weights[-1], label = 'preds')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PLeQb5xGGI2"
      },
      "source": [
        "**Определение (R2-score или коэффициент детерминации):**\n",
        "\n",
        "$$ R^2 = 1 - \\frac{MSE(y, \\widehat{y})}{D y} $$\n",
        "\n",
        "— это доля дисперсии зависимой переменной, объясняемая рассматриваемой моделью зависимости, то есть объясняющими переменными. Более точно — это единица минус доля необъяснённой дисперсии (дисперсии случайной ошибки модели, или условной по факторам дисперсии зависимой переменной) в дисперсии зависимой переменной. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VykDMvHG2zHE"
      },
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "train_preds = predict(X_train, weights)\n",
        "test_preds = predict(X_test, weights)\n",
        "\n",
        "print('train r2', r2_score(y_train, train_preds))\n",
        "print('test r2', r2_score(y_test, test_preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VEiNra22QmH"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "train_preds = predict(X_train, weights)\n",
        "test_preds = predict(X_test, weights)\n",
        "\n",
        "print('train mse', mean_squared_error(y_train, train_preds))\n",
        "print('test mse', mean_squared_error(y_test, test_preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G276_whHL02N"
      },
      "source": [
        "### Ridge&Lasso\n",
        "\n",
        "На практике точные формулы для подсчета коэффициентов линейной регрессии не используются, а используется метод градиентного спуска который состоит в подсчете производных от ошибки и шагу в направлении наискорейшего убывания функции (напомню, что мы стремимся минимизировать функцию потерь). Эти методы работают быстрее, чем точное вычисление обратных матриц и их перемножение.\n",
        "Более того, во многих задачах это единственный способ обучить модель, так как не всегда (на самом деле почти никогда) удается выписать точную формулу для минимума сложного функционала ошибки.\n",
        "\n",
        "Давайте рассмотрим реализации линейных регрессоров в библиотеке sklearn\n",
        "\n",
        "Но сначала давайте поймём, зачем вообще нужна регуляризация. Рассмотрим проблему мультиколлинеарности. В упрощённом понимании, это означает, что признаки линейно зависимы. Посмотрим, к чему это ведёт."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClvBLotDHCVt"
      },
      "source": [
        "X_adversary = X_train.copy()\n",
        "X_adversary[:, 0] = 2\n",
        "print(X_train.shape, X_adversary.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-xlOw0FyhlP"
      },
      "source": [
        "w_adversary = fit(X_adversary, y_train)\n",
        "w_adversary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1iQ4tbeyuN0"
      },
      "source": [
        "**ВОПРОС** Почему так произошло??"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Pic637Cy4nm"
      },
      "source": [
        "#TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTUfDWr4Iwxr"
      },
      "source": [
        "**Что произошло**\n",
        "\n",
        "Ранг матрицы $\\mathrm{X_{adversary}}$ равен 1, а размерность признакового пространства равна 2. Из линейной алгебры известно, что ранг произведения матриц не превосходит минимального ранга этих двух матриц: \n",
        "\n",
        "$$\n",
        "    \\mathrm{rk} AB \\leq \\min \\{\\mathrm{rk} A, \\mathrm{rk} B\\}\n",
        "$$\n",
        "\n",
        "К чему это здесь приводит? Посмотрим на аналитическое решение\n",
        "$$\n",
        "    w = (X^{\\top} X)^{-1} X^{\\top} y\n",
        "$$\n",
        "\n",
        "Нас интересует компонента $(X^{\\top} X)^{-1}$. Здесь обратная не определена, потому что ранг матрицы $X^{\\top} X$ не превосходит единицы. При этом матрица $X^{\\top} X$ -- квадратная, размера 2x2. Из линейной алгебры мы знаем, что квадратная матрица обратима только тогда, когда она полноранговая. Именно об этом Вам сигналит ошибка LinAlgError: Вы пытаетесь обратить матрицу, которую обращать нельзя. Что делать?\n",
        "\n",
        "**Определение ($L_{p}$-регуляризация):**\n",
        "Пусть задана линейная регрессия с вектором весов $w$ и функцией ошибок $\\mathcal{L}(y, \\widehat{y}(w))$ (например, среднеквадратичная ошибка), тогда к ней можно добавить $L_{p}$-регуляризацию изменив функционал ошибки следующим образом:\n",
        "\n",
        "$$\n",
        "    \\mathcal{L}(y, \\widehat{y}(w)) + \\alpha \\|w\\|_{p}^{p} \\to \\min\\limits_{w}\n",
        "$$\n",
        "\n",
        "Если $p$ равно 2, то это называют **Ridge**-регуляризацией, а если $p$ равно 1, то **Lasso**-регуляризацией.\n",
        "\n",
        "**Утверждение:** $L_{2}$-регуляризация позволяет избежать этой проблемы ($L_{p}$-нормы для произвольных $p$, на самом деле, тоже, но это уже нетривиально доказывать).\n",
        "\n",
        "**Определение (напоминание):** Собственные значения матрицы $A$ это такие числа $\\lambda$, что существует ненулевой вектор $x$, такой что $Ax = \\lambda x$\n",
        "\n",
        "А как вообще понять, насколько плохо всё с матрицей в плане того, что с ней будет происходить при попытке её обратить?\n",
        "Посчитать коэффициент обусловленности: отношение максимального собственного значения к минимальному. Чем он больше, тем всё хуже."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW79B9eNPHYf"
      },
      "source": [
        "eigenvals, eigenvectors = np.linalg.eig(X.reshape(1,-1).T @ X.reshape(1,-1))\n",
        "eigenvals.max() / eigenvals.min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Решим задачу регрессии с библиотекой sklearn"
      ],
      "metadata": {
        "id": "QWyCxLb7FG_t"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5x9U-gELRjY"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgaA6qihMav8"
      },
      "source": [
        "from sklearn.datasets import load_wine"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTf8-WxNMhF7"
      },
      "source": [
        "wine_data = load_wine()\n",
        "wine_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv9QhZ29Mb50"
      },
      "source": [
        "X = pd.DataFrame(wine_data['data'], columns=wine_data['feature_names'])\n",
        "y = wine_data['target']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X3jgBGuMeDE"
      },
      "source": [
        "_ = X.hist(X.columns, figsize=(10, 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5WwzMmiMfyr"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize = (10,6))\n",
        "sns.heatmap(X.corr())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn3C6aLWNg-E"
      },
      "source": [
        "sns.clustermap(X.corr())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHde418VN8cR"
      },
      "source": [
        "X.corr().loc['total_phenols', 'flavanoids']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZewQhvWnMipr"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=99, stratify=y\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjfmaMKIMlWT"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFdldjDfMsGD"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaslBvEsMnKj"
      },
      "source": [
        "regressor = LinearRegression()\n",
        "\n",
        "regressor.fit(X_train, y_train)\n",
        "test_predictions = regressor.predict(X_test)\n",
        "\n",
        "print('test mse: ', mean_squared_error(y_test, test_predictions))\n",
        "print('r2 score: ', r2_score(y_test, test_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eGcAHp4UiMq"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUrN2soBMoxb"
      },
      "source": [
        "plt.figure(figsize=(20, 8))\n",
        "plt.bar(X.columns, regressor.coef_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBT8gghkM0aI"
      },
      "source": [
        "Теперь обратимся к методам с регуляризацией.\n",
        "\n",
        "Ridge (L2-регуляризация) сильно штрафует за слишком большие веса и не очень за малые. При увеличении коэффициента перед регуляризатором веса меняются плавно"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4OT2jMXMzz8"
      },
      "source": [
        "alphas = np.linspace(1, 1000, 100)\n",
        "\n",
        "weights = np.empty((len(X.columns), 0))\n",
        "for alpha in alphas:\n",
        "    ridge_regressor = Ridge(alpha)\n",
        "    ridge_regressor.fit(X_train, y_train)\n",
        "    weights = np.hstack((weights, ridge_regressor.coef_.reshape(-1, 1)))\n",
        "plt.plot(alphas, weights.T)\n",
        "plt.xlabel('regularization coef')\n",
        "plt.ylabel('weight value')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRt8FhcHM4f_"
      },
      "source": [
        "Lasso (L1) одинаково сильно штрафует малые и большие веса, поэтому при достаточно большом коэффициенте регуляризации многие признаки становятся равными нулю, при этом остаются только наиболее инфромативные. Этот факт можно использовать для решения задачи отбора признаков."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBqkQ24lM3lG"
      },
      "source": [
        "alphas = np.linspace(0.1, 1, 100)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "weights = np.empty((len(X.columns), 0))\n",
        "for alpha in alphas:\n",
        "    lasso_regressor = Lasso(alpha)\n",
        "    lasso_regressor.fit(X_train, y_train)\n",
        "    weights = np.hstack((weights, lasso_regressor.coef_.reshape(-1, 1)))\n",
        "plt.plot(alphas, weights.T)\n",
        "plt.xlabel('regularization coef')\n",
        "plt.ylabel('weight value')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNcemWQ4My2y"
      },
      "source": [
        "ridge = Ridge(0.1)\n",
        "ridge.fit(X_train, y_train)\n",
        "print('\\n r2 score ridge: ', r2_score(y_test, ridge.predict(X_test)))\n",
        "print('test mse ridge: ', mean_squared_error(y_test, ridge.predict(X_test)))\n",
        "\n",
        "lasso = Lasso(0.1)\n",
        "lasso.fit(X_train, y_train)\n",
        "print('\\n r2 score lasso: ', r2_score(y_test, lasso.predict(X_test)))\n",
        "print('test mse lasso: ', mean_squared_error(y_test, lasso.predict(X_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGQcTMVoBnYP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}